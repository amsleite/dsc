{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Natural Language Processing</h1>\n",
    "\n",
    "<h2>Stanford Sentiment Treebank</h2>\n",
    "\n",
    "<p>A few years ago, Richard Socher put together a dataset for evaluating machine learning techniques to perform sentiment analysis. This dataset is interesting in that it includes a full parse tree for each sentence as well as a label representing how positive the sentiment is for that phrase for each branch in the parse tree.</p>\n",
    "\n",
    "<p>Socher's original version is spread across several files and requires some overly complicated parsing, so I just downloaded Kaggle's version of this dataset, which is a pre-parsed-and-merged version distributed as a single tsv file.</p>\n",
    "\n",
    "<p>If you'd like to play with this dataset, you can download it <a href=http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data>here</a></p>\n",
    "\n",
    "<p>To run this notebook, you will need:</p>\n",
    "<p>numpy</p>\n",
    "<p>scipy</p>\n",
    "<p>pandas</p>\n",
    "<p>matplotlib</p>\n",
    "<p>scikit-learn</p>\n",
    "<p>and, of course, ipython and the ipython notebook</p>\n",
    "\n",
    "<br>\n",
    "<p>To get started, let's load some common libraries for loading, transforming, and exploring text data...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's read in the file we downloaded from Kaggle. After I unzipped the file, I renamed it from \"train.tsv\" to \"movie_reviews_train.tsv\" to avoid name clashes with other datasets. It's stored under \"data\" in my home folder.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = pd.read_csv(os.path.join(os.environ['HOME'], 'data', 'movie_reviews_train.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To keep things simple, let's ignore the parse tree information. We'll only look at complete sentences and their corresponding labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases['word_count'] = phrases.Phrase.str.split().apply(len)\n",
    "phrases = phrases.set_index(['SentenceId', 'word_count']).sort_index()\n",
    "sentences = phrases.groupby(level=[0]).tail(1).reset_index()\n",
    "phrases = phrases.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Socher's original paper that presented this dataset considers a 5-way classification problem with the following interpretation of the labels:</p>\n",
    "\n",
    "<h5>Sentiment:</h5>\n",
    "<p>0 -> Extremely negative</p>\n",
    "<p>1 -> Slightly negative</p>\n",
    "<p>2 -> Not negative or positive</p>\n",
    "<p>3 -> Slightly positive</p>\n",
    "<p>4 -> Extremely positive</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<p>Again, in the interest of simplicity, let's make this a binary classification problem. Socher did this in the original paper by assigning a \"positive\" label to label values of 3 and 4, and a \"negative\" label to label values of 0 and 1. In the binary framework, labels of 2 are excluded.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences.loc[sentences.Sentiment <= 1, 'binary_sentiment'] = 'negative'\n",
    "sentences.loc[sentences.Sentiment >= 3, 'binary_sentiment'] = 'positive'\n",
    "dataset = sentences.loc[sentences.binary_sentiment.notnull(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Always good practice to check the shape of your dataframe after filtering...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's what the dataframe looks like...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the interest of competition, Kaggle doesn't make the labels for the test set available on this dataset. We could get the original dataset from Socher's website, but as I mentioned before, this involves some parsing and joining operations. In the interest of keeping things simple, let's just split the training set into a smaller training set and a test set with a 70/30 split.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "\n",
    "train_idx, test_idx = iter(cv.ShuffleSplit(n=dataset.shape[0], n_iter=1, test_size=0.3, random_state=123, indices=False)).next()\n",
    "dataset.loc[train_idx, 'model_set'] = 'train'\n",
    "dataset.loc[test_idx, 'model_set'] = 'test'\n",
    "assert dataset.model_set.isnull().sum() == 0\n",
    "train = dataset.loc[dataset.model_set=='train', :]\n",
    "test = dataset.loc[dataset.model_set=='test', :]\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's get some summary stats!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train.binary_sentiment.value_counts()\n",
    "print \"\"\n",
    "print \"Fraction positive instances:\\t{0}\".format((train.binary_sentiment=='positive').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's see if there's any low-hanging fruit in the word counts. I wouldn't expect the number of words in a sentence to be predictive of sentiment, but it only takes a minute to check...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 60, 20)\n",
    "pl.figure(figsize=(16, 12))\n",
    "pl.hist(train.loc[train.binary_sentiment=='positive', 'word_count'].values, histtype='stepfilled', color='b', alpha=0.5, normed=True, bins=bins)\n",
    "pl.hist(train.loc[train.binary_sentiment=='negative', 'word_count'].values, histtype='stepfilled', color='r', alpha=0.5, normed=True, bins=bins)\n",
    "pl.title('Word Counts -- positive and negative sentiment sentences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Hmmm... There's not a lot of signal in this variable. Maybe letters-per-word will look better?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 10, 20)\n",
    "train['letters_per_word'] = train.apply(lambda x: np.sum([len(word) for word in x.Phrase.split()]) / float(x.word_count), axis=1)\n",
    "test['letters_per_word'] = test.apply(lambda x: np.sum([len(word) for word in x.Phrase.split()]) / float(x.word_count), axis=1)\n",
    "pl.figure(figsize=(16, 12))\n",
    "pl.hist(train.loc[train.binary_sentiment=='positive', 'letters_per_word'].values, histtype='stepfilled', color='b', alpha=0.5, normed=True, bins=bins)\n",
    "pl.hist(train.loc[train.binary_sentiment=='negative', 'letters_per_word'].values, histtype='stepfilled', color='r', alpha=0.5, normed=True, bins=bins)\n",
    "pl.title('Letters Per Word -- positive and negative sentiment sentences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Slightly better, but still not seeing a ton of separation. Let's see if some machine learning can help us here...</p>\n",
    "\n",
    "<h2>Machine Learning with Text</h2>\n",
    "\n",
    "<p>The classic procedure for building discriminative models from text is to create a gigantic, sparse matrix where each column represents a word in your dictionary (i.e. a word that appears somewhere in your training set) and each row represents a document (in this case, a sentence). Each cell can represent:</p>\n",
    "<p>a. A binary indicator of whether or not that word is present in the sentence</p>\n",
    "<p>b. A count of the number of times the word appears in that sentence, or</p>\n",
    "<p>c. either \"a.\" or \"b.\" multiplied by some weighting scheme</p>\n",
    "<br>\n",
    "<p>We'll use tf/idf weighting, which is a popular choice for general natural language processing tasks. Usually binary indicators work better than word counts for sentiment analysis, so we'll use those for this example.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "vec = text.TfidfVectorizer(binary=True,\n",
    "                           lowercase=False) # lowercasing can reduce the column count, but we should see\n",
    "                                            #  how many columns we end up with first\n",
    "                           \n",
    "X = vec.fit_transform(train.Phrase.values)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is not unusual to end up with more columns than rows in the resulting matrix, so any machine learning algorithm that uses this matrix must:</p>\n",
    "<p>1. Scale to work with huge numbers of columns</p>\n",
    "<p>2. Not produce degenerate solutions when the number of columns >= the number of rows, and</p>\n",
    "<p>3. Be sufficiently regularized</p>\n",
    "<p>We'll assume that words that appear only once or twice in our training set are more likely to lead to overfitting than to a better model and leave them out. We'll also lowercase each word in an effort to reduce the number of columns, which should help with overfitting as well. Let's see what the resulting shape of this matrix is and make a decision on the best algorithm to use to build a model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = text.TfidfVectorizer(binary=True,\n",
    "                           lowercase=True,\n",
    "                           min_df = 3, # words must appear at least 3 times to be included in the matrix\n",
    "                           norm=u'l2', # taking the L2 norm of each row after weighting is a trick that usually helps\n",
    "                           use_idf=True, # this applies the weighting, which also usually helps\n",
    "                           smooth_idf=True) # matters when you're considering low word counts, which we are here\n",
    "X = vec.fit_transform(train.Phrase.values)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we have 1/3 the number of columns as before, and we have fewer columns than rows. Despite the improvement, we still have a lot of columns, so we still probably want to start with something that is both sufficiently regularized and scales well to huge numbers of columns. Penalized logistic regression is a popular approach. L2 regularization is generally an easier and faster optimization than L1, so let's start with that.</p>\n",
    "<p>We'll do a grid search for our regularization hyperparameter. Since accuracy is our metric, we can use lots of folds and still get a good estimate on out-of-sample performance while keeping our training set size big. We also won't worry about stratifying our folds because class imbalance is almost non-existant on this dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search as gs\n",
    "from sklearn import pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "param_grid = [{'model__C': 2.**np.linspace(-15, 15, 20)}]\n",
    "folds = cv.KFold(n=train.shape[0],\n",
    "                 n_folds=5,\n",
    "                 shuffle=True,\n",
    "                 indices=False,\n",
    "                 random_state=1234)\n",
    "model = lm.LogisticRegression(penalty='l2',\n",
    "                              dual=False, #this is usually faster when the column count is higher than the row count\n",
    "                              random_state=567)\n",
    "pipe = pipeline.Pipeline([('vec', vec), ('model', model)])\n",
    "searcher = gs.GridSearchCV(pipe, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           iid=True, # iid is never a great assumption, but it's not a horrible one here\n",
    "                           cv=folds,\n",
    "                           refit=False,\n",
    "                           n_jobs=-1) # parallelize the grid search across all cores\n",
    "searcher.fit(train.Phrase.values, train.binary_sentiment.values)\n",
    "print \"Best C:\\t\\t{0}\".format(np.log2(searcher.best_params_['model__C']))\n",
    "print \"Best Score:\\t{0}\".format(searcher.best_score_)\n",
    "\n",
    "pipe.named_steps['model'].C = searcher.best_params_['model__C']\n",
    "for fold, (train_idx, test_idx) in enumerate(folds):\n",
    "    pipe.fit(train.loc[train_idx, 'Phrase'].values, train.loc[train_idx, 'binary_sentiment'].values)\n",
    "    train.loc[test_idx, 'fold'] = fold\n",
    "    train.loc[test_idx, 'unigram_score'] = pipe.predict_proba(train.loc[test_idx, 'Phrase'].values)[:, 1]\n",
    "pipe.fit(train.Phrase.values, train.binary_sentiment.values)\n",
    "test['unigram_score'] = pipe.predict_proba(test.Phrase.values)[:, 1]\n",
    "bins = np.linspace(0., 1., 20)\n",
    "pl.figure(figsize=(16, 12))\n",
    "pl.hist(train.loc[train.binary_sentiment=='positive', 'unigram_score'].values, histtype='stepfilled', color='b', alpha=0.5, normed=True, bins=bins)\n",
    "pl.hist(train.loc[train.binary_sentiment=='negative', 'unigram_score'].values, histtype='stepfilled', color='r', alpha=0.5, normed=True, bins=bins)\n",
    "pl.title('Unigram Score -- positive and negative sentiment sentences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Pretty substantial improvement, right? To be fair, the \"C\" hyperparameter was chosen based on the one that produced the best accuracy score over these cross validation folds. There is a small amount of data snooping present in the score, but since only one hyperparameter was chosen, I think it's safe to say that these scores are fairly representative of what we can expect to see on out-of-sample data.</p>\n",
    "\n",
    "<h3>N-Grams</h3>\n",
    "<p>One problem with the bag-of-words model is that we lose all the context for the words. As an example, consider the following sentences:</p>\n",
    "\n",
    "<h5>Jack loves Jill.</h5>\n",
    "<h5>Jill loves Jack.</h5>\n",
    "\n",
    "<p>These sentences produce identical bag-of-words vector representations despite having very different meanings. One way that we might restore some context into the vector representation is to take each pair of words in addition to each single word. In the example above, we'd consider columns ['jack', 'loves', 'jill'] for each sentence, but the first sentence would have additional columns ['jack loves', 'loves jill'] and the second sentence would have additional columns ['jill loves', 'loves jack']. This is controlled with the \"ngram_range\" hyperparameter in the tf/idf vectorizer. Using bigrams, trigrams, etc... usually yields improvements in natural language processing tasks, although you should expect diminishing returns and overfitting at some point. Let's see if we get any improvement in our model's performance...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = text.TfidfVectorizer(binary=True,\n",
    "                           lowercase=True,\n",
    "                           ngram_range=(1, 2),\n",
    "                           min_df = 3, # words must appear at least 3 times to be included in the matrix\n",
    "                           norm=u'l2', # taking the L2 norm of each row after weighting is a trick that usually helps\n",
    "                           use_idf=True, # this applies the weighting, which also usually helps\n",
    "                           smooth_idf=True)\n",
    "\n",
    "param_grid = [{'model__C': 2.**np.linspace(-15, 15, 20)}]\n",
    "folds = cv.KFold(n=train.shape[0],\n",
    "                 n_folds=5,\n",
    "                 shuffle=True,\n",
    "                 indices=False,\n",
    "                 random_state=1234)\n",
    "model = lm.LogisticRegression(penalty='l2',\n",
    "                              dual=True, #this is usually faster when the column count is higher than the row count\n",
    "                              random_state=567)\n",
    "pipe = pipeline.Pipeline([('vec', vec), ('model', model)])\n",
    "searcher = gs.GridSearchCV(pipe, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           iid=True, # iid is never a great assumption, but it's not a horrible one here\n",
    "                           cv=folds,\n",
    "                           refit=False,\n",
    "                           n_jobs=-1) # parallelize the grid search across all cores\n",
    "searcher.fit(train.Phrase.values, train.binary_sentiment.values)\n",
    "print \"Best C:\\t\\t{0}\".format(np.log2(searcher.best_params_['model__C']))\n",
    "print \"Best Score:\\t{0}\".format(searcher.best_score_)\n",
    "\n",
    "pipe.named_steps['model'].C = searcher.best_params_['model__C']\n",
    "for fold, (train_idx, test_idx) in enumerate(folds):\n",
    "    pipe.fit(train.loc[train_idx, 'Phrase'].values, train.loc[train_idx, 'binary_sentiment'].values)\n",
    "    train.loc[test_idx, 'fold'] = fold\n",
    "    train.loc[test_idx, 'bigram_score'] = pipe.predict_proba(train.loc[test_idx, 'Phrase'].values)[:, 1]\n",
    "pipe.fit(train.Phrase.values, train.binary_sentiment.values)\n",
    "test['bigram_score'] = pipe.predict_proba(test.Phrase.values)[:, 1]\n",
    "bins = np.linspace(0., 1., 20)\n",
    "pl.figure(figsize=(16, 12))\n",
    "pl.hist(train.loc[train.binary_sentiment=='positive', 'bigram_score'].values, histtype='stepfilled', color='b', alpha=0.5, normed=True, bins=bins)\n",
    "pl.hist(train.loc[train.binary_sentiment=='negative', 'bigram_score'].values, histtype='stepfilled', color='r', alpha=0.5, normed=True, bins=bins)\n",
    "pl.title('Bigram Score -- positive and negative sentiment sentences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Well, we got a slight improvement, but not as big of one as we might have expected. Let's see what the shape of our document term matrix looks like with the additional columns...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigramX = vec.fit_transform(train.Phrase.values)\n",
    "bigramX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looks like we have almost twice as many columns as before...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.data.shape[0] / float(np.prod(X.shape))\n",
    "print bigramX.data.shape[0] / float(np.prod(bigramX.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And there lies the problem -- these additional bigram columns aren't as dense (they don't have as many non-zero components) as the unigram columns. This task is particularly challenging in that each document contains a single sentence, so sparsity is a problem even when only considering only unigrams. Looking at the above, even the unigram matrix only has 0.35% non-zero components. It's amazing that the algorithm was able to model this at all!</p>\n",
    "\n",
    "<h4>For another discussion...</h4>\n",
    "<p>Let's talk about how more modern techniques that attempt to solve the sparsity problem by rethinking the bag-of-words approach.</p>\n",
    "\n",
    "<h3>Ensembling</h3>\n",
    "<p>In the meantime, let's ensemble our two models. There are typically two approaches:</p>\n",
    "<p>1. simple average</p>\n",
    "<p>2. an ensemble model that combines the scores of the submodels</p>\n",
    "<br>\n",
    "<p>Since the simple average is simpler than the model, let's try that first:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Simple average accuracy: {0}\".format(metrics.accuracy_score(train.binary_sentiment.values, np.where(train[['unigram_score', 'bigram_score']].mean(axis=1) > 0.5, 'positive', 'negative')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Not bad. We got a small improvement. Let's see if we do better with an ensemble model (a.k.a. a \"stack\").</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [{'C': 2.**np.linspace(-15, 15, 20)}]\n",
    "folds = cv.KFold(n=train.shape[0],\n",
    "                 n_folds=5,\n",
    "                 shuffle=True,\n",
    "                 indices=False,\n",
    "                 random_state=1234)\n",
    "model = lm.LogisticRegression(penalty='l2',\n",
    "                              dual=False, #this is usually faster when the column count is higher than the row count\n",
    "                              random_state=567)\n",
    "searcher = gs.GridSearchCV(model, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           iid=True, # iid is never a great assumption, but it's not a horrible one here\n",
    "                           cv=folds,\n",
    "                           refit=True,\n",
    "                           n_jobs=-1) # parallelize the grid search across all cores\n",
    "searcher.fit(train[['unigram_score', 'bigram_score']].values, train.binary_sentiment.values)\n",
    "print \"Best C:\\t\\t{0}\".format(np.log2(searcher.best_params_['C']))\n",
    "print \"Best Ensemble Score:\\t{0}\".format(searcher.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Even better! We'll use the stack to score our holdout set and see how well we expect to do on evaluating sentiment for sentences from movie reviews.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['ensemble_score'] = searcher.predict_proba(test[['unigram_score', 'bigram_score']].values)[:, 1]\n",
    "print \"Ensemble Accuracy on Holdout Set: {0}\".format(metrics.accuracy_score(test.binary_sentiment.values,\n",
    "                                                                            searcher.predict(test[['unigram_score', 'bigram_score']].values)))\n",
    "bins = np.linspace(0., 1., 20)\n",
    "pl.figure(figsize=(16, 12))\n",
    "pl.hist(test.loc[test.binary_sentiment=='positive', 'ensemble_score'].values, histtype='stepfilled', color='b', alpha=0.5, normed=True, bins=bins)\n",
    "pl.hist(test.loc[test.binary_sentiment=='negative', 'ensemble_score'].values, histtype='stepfilled', color='r', alpha=0.5, normed=True, bins=bins)\n",
    "pl.title('Ensemble Scores on Holdout Set');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
